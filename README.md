# LLM-Agent-Benchmark



## :book: Introduction

In the swiftly evolving landscape of artificial intelligence, Large Language Models (LLMs) have emerged as a pivotal cornerstone, revolutionizing how we interact with and harness the power of natural language processing.  However, as LLMs gain widespread application in both research and industry sectors, the imperative shifts towards evaluating their efficacy rather than perpetuating a cycle of unbridled performance iterations. This paradigm shift raises critical questions: *i) what to evaluate?  ii) where to evaluate? iii)How to evaluate?* Diverse research endeavors have proposed varying interpretations and methodologies in response to these queries. The aim of this work is to methodically review and organize benchmarks that are both LLMs and agent-powered, thereby providing a streamlined resource for those journeying towards Artificial General Intelligence (AGI).



## :dizzy: List

- [2023/12] **A Survey on Evaluation of Large Language Models.** *Yupeng Chang ( Jilin University) et al. arXiv.* [[paper](https://arxiv.org/pdf/2307.03109.pdf)] [[project page](https://github.com/MLGroupJLU/LLM-eval-survey)]

- [2023/11] **Benchmarking Foundation Models with Language-Model-as-an-Examiner.** *Yushi Bai (Tsinghua) et al. arXiv.* [[paper](https://arxiv.org/pdf/2306.04181.pdf)]
- [2023/08] **Large Language Models are not Fair Evaluators.** *Peiyi Wang (Peking) et al. arXiv.* [[paper](https://arxiv.org/pdf/2305.17926.pdf)] [[project page](https://github.com/i-Eval/FairEval)]

- [2023/09] **Evaluating Cognitive Maps and Planning in Large Language Models with CogEval.** *Ida Momennejad (Microsoft)  et al. arXiv.*  [[paper](https://arxiv.org/pdf/2309.15129.pdf)]
- [2023/04] **SocialDial: A Benchmark for Socially-Aware Dialogue Systems.** *Haolan Zhan (Monash) et al. SIGIR.* [[paper](https://arxiv.org/pdf/2304.12026.pdf)] [[project page](https://github.com/zhanhl316/SocialDial)]
- [2023/07] **Emotional Intelligence of Large Language Models.** *Xuena Wang (Tsinghua) et al. arXiv.* [[paper](https://arxiv.org/ftp/arxiv/papers/2307/2307.09042.pdf)]

----------------------

#### ToolUse

- [2023/10] **API-Bank: A Comprehensive Benchmark for Tool-Augmented LLMs.** *Minghao Li (Alibaba) et al. arXiv.* [[paper](https://arxiv.org/pdf/2304.08244.pdf)] [[project page](https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/api-bank)]
- [2023/05] **On the Tool Manipulation Capability of Open-source Large Language Models.** *Qiantong Xu (SambaNova) et al. arXiv.* [[paper](https://arxiv.org/pdf/2305.16504.pdf)] [[project page](https://github.com/sambanova/toolbench)]
- [2023/10] **ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs.**  *Yujia Qin (Tsinghua) et al. arXiv.* [[paper](https://arxiv.org/pdf/2307.16789.pdf)] [[project page](https://github.com/OpenBMB/ToolBench)]

---------------

#### Multimodal

- [2023/11] **M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models.** *Wenxuan Zhang (Alibaba) et al. arXiv.* [[paper](https://arxiv.org/pdf/2306.05179.pdf)] [[project page](https://github.com/DAMO-NLP-SG/M3Exam?tab=readme-ov-file)]

- [2023/11] **A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity.** *Yejin Bang (CAiRE) et al. arXiv.* [[paper](https://arxiv.org/pdf/2302.04023.pdf)] [[project page](https://github.com/HLTCHKUST/chatgpt-evaluation)]
- [2023/12] **MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models.** *Chaoyou Fu (Tencent) et al. arXiv.* [[paper](https://arxiv.org/pdf/2306.13394.pdf)] [[project page](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation)]

- [2024/01] **Q-Bench: A Benchmark for General-Purpose Foundation Models on Low-level Vision.** *Haoning Wu (Nanyang Technological University) et al. arXiv.* [[paper](https://arxiv.org/abs/2309.14181)] [[project page](https://github.com/Q-Future/Q-Bench?tab=readme-ov-file)]

----------------------------

#### Agent

- [2023/10] **AgentBench: Evaluating LLMs as Agents.**  *Xiao Liu (Tsinghua) et al. arXiv.* [[paper](https://arxiv.org/abs/2308.03688)] [[project page](https://github.com/THUDM/AgentBench)]
- [2023/08] **AgentSims: An Open-Source Sandbox for Large Language Model Evaluation.** *Jiaju Lin (PTA Studio) et al. arXiv.* [[paper](https://arxiv.org/pdf/2308.04026.pdf)] 

-----------------------------------

#### Dataset

- [2023/10] **Investigating Table-to-Text Generation Capabilities of LLMs in Real-World Information Seeking Scenarios.** *Yilun Zhao (Yale) EMNLP.* [[paper](https://aclanthology.org/2023.emnlp-industry.17.pdf)] [[project page](https://github.com/yale-nlp/LLM-T2T)]
- [2023/10] **Empower Large Language Model to Perform Better on Industrial Domain-Specific Question Answering.** *Fangkai Yang (Microsoft) EMNLP.*[[paper](https://arxiv.org/pdf/2305.11541.pdf)] [[project page](https://github.com/microsoft/Microsoft-Q-A-MSQA-)]

----------------------

#### ZhiHu

- [2023/11] **LLM Evaluation 如何评估一个大模型？** *Isaac 张雯轩* [[project page](https://zhuanlan.zhihu.com/p/644373658)]
- [2023/11] **基于LLM Agent评估研究** *龚旭东* [[project page](https://zhuanlan.zhihu.com/p/669325175)]



